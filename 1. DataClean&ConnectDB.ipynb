{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c117f679-bc10-4a3b-b440-49b9307c5198",
   "metadata": {},
   "source": [
    "# 1. Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "49bab712-e748-4cae-a1be-16c2e7ee3676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "import scipy.stats as st\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "06e01d00-223d-4ce1-b9b6-af5c8d47748f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/MEG_CHECK RELEASE/Notebook Files\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafbb914-2b35-4197-ae90-f5f0323cb4b4",
   "metadata": {},
   "source": [
    "# 2. Data-Cleaning our CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "ffb098cf-27dd-4662-a045-1a0bfd9a330c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAYEE                                                   object\n",
      "PROJECT                                                 object\n",
      "CHECK NO                                                object\n",
      "AMOUNT                                                 float64\n",
      "APV NO                                                  object\n",
      "CV NO                                                   object\n",
      "PO NO                                                   object\n",
      "COMPANY                                                 object\n",
      "DATE OF INCLUSION IN THE LIST OF FOR RELEASE    datetime64[ns]\n",
      "RELEASING VENUE                                         object\n",
      "BILLING NO.                                             object\n",
      "dtype: object\n",
      "PAYEE                                             0\n",
      "PROJECT                                           0\n",
      "CHECK NO                                          0\n",
      "AMOUNT                                            0\n",
      "APV NO                                            0\n",
      "CV NO                                             0\n",
      "PO NO                                             1\n",
      "COMPANY                                           0\n",
      "DATE OF INCLUSION IN THE LIST OF FOR RELEASE      0\n",
      "RELEASING VENUE                                   0\n",
      "BILLING NO.                                     157\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cleaning DataFrame from CSV\n",
    "def load_dataframe(file_path, output_path=\"cleaned.csv\"):\n",
    "    use_cols = [\n",
    "        'PAYEE', 'PROJECT', 'CHECK NO', 'AMOUNT', 'APV NO', 'CV NO',\n",
    "        'PO NO', 'COMPANY', 'DATE OF INCLUSION IN THE LIST OF FOR RELEASE',\n",
    "        'RELEASING VENUE'\n",
    "    ]\n",
    "\n",
    "    col_dtypes = {\n",
    "                    'PAYEE': str,\n",
    "                    'PROJECT': str,\n",
    "                    'CHECK NO': str,\n",
    "                    'APV NO': str,\n",
    "                    'CV NO': str,\n",
    "                    'PO NO': str,\n",
    "                    'COMPANY': str,\n",
    "                    'RELEASING VENUE': str,\n",
    "                    'PAYEE_CODE': str\n",
    "                }\n",
    "    \n",
    "    # Load depending on file type\n",
    "    if file_path.endswith(\".csv\"):\n",
    "        df = pd.read_csv(file_path, dtype=col_dtypes, skiprows=12, low_memory=True, usecols=use_cols)\n",
    "    elif file_path.endswith((\".xls\", \".xlsx\")):\n",
    "        df = pd.read_excel(file_path, usecols=use_cols, skiprows=12)\n",
    "        df = df.astype(col_dtypes)  # enforce dtypes manually\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Use CSV or Excel.\")\n",
    "\n",
    "    # Convert date column\n",
    "    df[\"DATE OF INCLUSION IN THE LIST OF FOR RELEASE\"] = pd.to_datetime(\n",
    "    df[\"DATE OF INCLUSION IN THE LIST OF FOR RELEASE\"].str.strip(),\n",
    "    format=\"%m/%d/%Y\", errors=\"coerce\")\n",
    "    \n",
    "    # Clean rows\n",
    "    df_clean = df[df[\"DATE OF INCLUSION IN THE LIST OF FOR RELEASE\"].notna()].copy()\n",
    "    df_clean = df_clean[~(df_clean == df_clean.columns).all(axis=1)]\n",
    "\n",
    "    df_clean[\"AMOUNT\"] = (\n",
    "                        df_clean[\"AMOUNT\"]\n",
    "                        .astype(str)                       # ensure string\n",
    "                        .str.replace(\",\", \"\", regex=True)  # remove commas\n",
    "                        .str.replace(r\"[^\\d.]\", \"\", regex=True)  # strip non-numeric except .\n",
    "                        .replace(\"\", np.nan)               # turn empty strings into NaN\n",
    "                        .astype(float)                     # convert to float\n",
    "                    )\n",
    " \n",
    "    # Split PAYEE into PAYEE and PAYEE_CODE\n",
    "    df_clean[\"BILLING NO.\"] = df_clean[\"PAYEE\"].str.extract(r\"\\((.*?)\\)\")\n",
    "    df_clean[\"PAYEE\"] = df_clean[\"PAYEE\"].str.replace(r\"\\s*\\(.*?\\)\", \"\", regex=True)\n",
    "\n",
    "    # Checking if Column Types\n",
    "    col_dtype_map = df_clean.dtypes\n",
    "    print(col_dtype_map)\n",
    "    print(df_clean.isna().sum())\n",
    "\n",
    "    df_clean.to_csv(output_path, index=False)\n",
    "    return df_clean\n",
    "    \n",
    "# Using function load_dataframe\n",
    "df_clean = load_dataframe(r\"/workspace/MEG_CHECK RELEASE/RAW_DATASETS/JANUARY 2026/List of checks for release January 30, 2026.csv\", \n",
    "                          output_path=r\"/workspace/MEG_CHECK RELEASE/CLEANED_DATASETS/Meg_CheckRelease_01-30-2026.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f255b70a-b0fb-481b-a7ad-a41473b3dca3",
   "metadata": {},
   "source": [
    "## 2.1 Cleaning Amount due to more than 1 \".\" in a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "83390d65-6e0a-4984-8941-f7f9e15bc25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         PAYEE         PROJECT     CHECK NO  \\\n",
      "344  MORETHANALL INDUSTRIAL TRADING INC. (B#2)  9 Central Park  CHECKWRITER   \n",
      "\n",
      "            AMOUNT         APV NO          CV NO          PO NO  \\\n",
      "344  1,152.634.20   012026-00032   012026-00032   NWI-000000708   \n",
      "\n",
      "                     COMPANY DATE OF INCLUSION IN THE LIST OF FOR RELEASE  \\\n",
      "344  NORTHWIN PROPERTIES INC                                   01/30/2026   \n",
      "\n",
      "                                       RELEASING VENUE  \n",
      "344  UBP MCKINLEY WEST â€“ LAWTON AVENUE LOWER G/F, R...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28/1819231755.py:7: DtypeWarning: Columns (1,3,4,5,6,7,8,9,10,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"/workspace/MEG_CHECK RELEASE/RAW_DATASETS/JANUARY 2026/List of checks for release January 30, 2026.csv\",\n"
     ]
    }
   ],
   "source": [
    "use_cols = [\n",
    "        'PAYEE', 'PROJECT', 'CHECK NO', 'AMOUNT', 'APV NO', 'CV NO',\n",
    "        'PO NO', 'COMPANY', 'DATE OF INCLUSION IN THE LIST OF FOR RELEASE',\n",
    "        'RELEASING VENUE'\n",
    "    ]\n",
    "\n",
    "df = pd.read_csv(\"/workspace/MEG_CHECK RELEASE/RAW_DATASETS/JANUARY 2026/List of checks for release January 30, 2026.csv\", \n",
    "                 skiprows=14, low_memory=True, usecols=use_cols)\n",
    "\n",
    "# cant = df.loc[df[\"AMOUNT\"] == \"1152.634.20\"]\n",
    "# print(cant)\n",
    "\n",
    "print(df[df[\"AMOUNT\"].str.count(r\"\\.\") > 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0a214f-02ee-4ec2-a964-39b5336c97ac",
   "metadata": {},
   "source": [
    "## 2.2 Recursive Cleaning Before Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "b3dd304a-7af8-43ef-a1b5-766ae7286547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added Meg_CheckRelease_01-23-2026.csv with 534 rows to merge\n",
      "Added Meg_CheckRelease_01-16-2026.csv with 691 rows to merge\n",
      "Added Meg_CheckRelease_01-30-2026.csv with 417 rows to merge\n",
      "Added Meg_CheckRelease_01-09-2026.csv with 591 rows to merge\n",
      "Merged CSV saved to /workspace/MEG_CHECK RELEASE/CLEANED_DATASETS_JAN.csv with 2233 rows\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "raw_csv_path = \"MEG_CHECK RELEASE/RAW_DATASETS\"\n",
    "cleaned_output_path = \"MEG_CHECK RELEASE/CLEANED_DATASETS\"\n",
    "merged_output_file = \"MEG_CHECK RELEASE/MERGED_DATA.csv\"\n",
    "\n",
    "def recursive_load_dataframe(raw_csv_path=raw_csv_path, cleaned_output_path=cleaned_output_path):\n",
    "    \"\"\"\n",
    "    Recursively load all CSVs from raw_csv_path,\n",
    "    save copies to cleaned_output_path.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(cleaned_output_path):\n",
    "        os.makedirs(cleaned_output_path)\n",
    "    \n",
    "    for root, dirs, files in os.walk(raw_csv_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(\".csv\") and \"-checkpoint\" not in file.lower():\n",
    "                csv_file_path = os.path.join(root, file)\n",
    "                print(f\"Found CSV: {csv_file_path}\")\n",
    "                \n",
    "                try:\n",
    "                    df = pd.read_csv(csv_file_path)\n",
    "                    print(f\"Loaded {len(df)} rows from {file}\")\n",
    "                    \n",
    "                    # Save copy to cleaned_output_path\n",
    "                    output_file_path = os.path.join(cleaned_output_path, file)\n",
    "                    df.to_csv(output_file_path, index=False)\n",
    "                    print(f\"Saved to {output_file_path}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to load {csv_file_path}: {e}\")\n",
    "\n",
    "def merge_all_csvs(cleaned_output_path=cleaned_output_path, merged_output_file=merged_output_file):\n",
    "    \"\"\"\n",
    "    Merge only main CSV files (ignore checkpoint files).\n",
    "    \"\"\"\n",
    "    all_dfs = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(cleaned_output_path):\n",
    "        for file in files:\n",
    "            # Ignore checkpoint files\n",
    "            if file.lower().endswith(\".csv\") and \"-checkpoint\" not in file.lower():\n",
    "                \n",
    "                csv_file_path = os.path.join(root, file)\n",
    "                \n",
    "                try:\n",
    "                    df = pd.read_csv(csv_file_path)\n",
    "                    all_dfs.append(df)\n",
    "                    print(f\"Added {file} with {len(df)} rows to merge\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to read {csv_file_path}: {e}\")\n",
    "    \n",
    "    if all_dfs:\n",
    "        merged_df = pd.concat(all_dfs, ignore_index=True)\n",
    "        merged_df.to_csv(merged_output_file, index=False)\n",
    "        print(f\"Merged CSV saved to {merged_output_file} with {len(merged_df)} rows\")\n",
    "    else:\n",
    "        print(\"No CSV files found to merge.\")\n",
    "\n",
    "# Step 1: Load CSVs recursively and save copies of each file\n",
    "# recursive_load_dataframe(raw_csv_path, cleaned_output_path)\n",
    "\n",
    "# Step 2: Merge all cleaned CSVs into one CSV\n",
    "merge_all_csvs(\"/workspace/MEG_CHECK RELEASE/CLEANED_DATASETS/JANUARY 2026\", \"/workspace/MEG_CHECK RELEASE/CLEANED_DATASETS_JAN.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1ed1c8-9eff-49f3-baac-ceffd7e7c40b",
   "metadata": {},
   "source": [
    "# 3. Connecting to PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "aebcae42-089b-41fc-bd3d-511c09c8cf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "f990ac33-4ea6-4758-8d05-98e766b46851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "with psycopg2.connect(\n",
    "    host=\"host.docker.internal\",\n",
    "    port=5432,\n",
    "    database=\"MegaworldDB\",\n",
    "    user=\"admin\",\n",
    "    password=\"123456789\",\n",
    "    sslmode=\"prefer\"\n",
    ") as conn:\n",
    "    # Use a context manager for the cursor\n",
    "    with conn.cursor() as cur:\n",
    "        csv_file_path = \"/workspace/MEG_CHECK RELEASE/CLEANED_DATASETS_FEB_2026.csv\"\n",
    "        \n",
    "        # Import CSV into table\n",
    "        with open(csv_file_path, 'r') as f:\n",
    "            cur.copy_expert(\n",
    "                \"\"\"\n",
    "                COPY check_release.records(\n",
    "                    \"PAYEE\",\n",
    "                    \"PROJECT\",\n",
    "                    \"CHECK NO\",\n",
    "                    \"AMOUNT\",\n",
    "                    \"APV NO\",\n",
    "                    \"CV NO\",\n",
    "                    \"PO NO\",\n",
    "                    \"COMPANY\",\n",
    "                    \"DATE OF INCLUSION IN THE LIST OF FOR RELEASE\",\n",
    "                    \"RELEASING VENUE\",\n",
    "                    \"BILLING NO.\"\n",
    "                )\n",
    "                FROM STDIN WITH CSV HEADER\n",
    "                \"\"\",\n",
    "                f\n",
    "            )\n",
    "        \n",
    "        # Commit is automatic when using 'with conn', but explicit commit is fine\n",
    "        conn.commit()\n",
    "\n",
    "print(\"CSV imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d0b2f0-2b35-4cb0-b977-eccb8a9d3845",
   "metadata": {},
   "outputs": [],
   "source": [
    "with conn.cursor() as cur:\n",
    "            cur.execute(\"\"\"\n",
    "                CREATE SCHEMA IF NOT EXISTS Check_Release;\n",
    "\n",
    "                CREATE TABLE IF NOT EXISTS Check_Release.Records (\n",
    "                    \"PAYEE\" TEXT,\n",
    "                    \"PROJECT\" TEXT,\n",
    "                    \"CHECK NO\" TEXT,\n",
    "                    \"AMOUNT\" NUMERIC,\n",
    "                    \"APV NO\" TEXT,\n",
    "                    \"CV NO\" TEXT,\n",
    "                    \"PO NO\" TEXT,\n",
    "                    \"COMPANY\" TEXT,\n",
    "                    \"DATE OF INCLUSION IN THE LIST OF FOR RELEASE\" DATE,\n",
    "                    \"RELEASING VENUE\" TEXT,\n",
    "                    \"BILLING NO.\" TEXT\n",
    "                );\n",
    "            \"\"\")\n",
    "            conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c57963-5bd0-4bc1-aa43-01b4d4301d16",
   "metadata": {},
   "source": [
    "# **WE CAN NOW PROCEED OUR ANALYSIS IN POSTGRESQL - http://localhost:5050/browser/**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
